\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Questions}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Programming Assignment}{2}}
\citation{fayyad1992}
\@writefile{toc}{\contentsline {section}{\numberline {3}Decision Trees}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results}}{5}}
\newlabel{tab:result}{{1}{5}}
\bibcite{fayyad1992}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of the entropy between the training data and the test data: The splitting by the 9th attribute on the training data achieves very low entroy, whose weighted average is 0.35332. This is why the 9th attribute was selected as the root node for the decision treee. On the other hand, the splitting by the same attribute on the test data results in very high entroy, whose weighted average is 0.99671. This indicates that the distribution of the 9th values against the labels are significantly different between the training data and the test data, and this causes very poor performance of the decision tree on the test data.}}{6}}
\newlabel{fig:comparison}{{1}{6}}
